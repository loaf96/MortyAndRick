{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from pymongo import MongoClient\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objects as go\n",
    "import tensorflow\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all of necessary databases that will be used to learn off of\n",
    "\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client['toboR_brain']\n",
    "leon = db['Leon']\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('appos.json') as apo:\n",
    "    huh = json.load(apo)\n",
    "    \n",
    "wuh = huh['appos']\n",
    "wuh['''we'll'''] = 'we will'\n",
    "\n",
    "with open('contractions.json', 'r') as contr:\n",
    "    tnoc = json.load(contr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smrt():\n",
    "    morty = list(leon.find({},{'_id':0, 'line':1}))\n",
    "    ri = leon.distinct('name')\n",
    "    ck = ' '.join(ri)\n",
    "    rick = str()\n",
    "    for y in morty:\n",
    "        rick += y['line'] + ' \\n'\n",
    "\n",
    "    rick += ck\n",
    "    jerry = rick.split(' ')\n",
    "    beth = [x.replace('(', '').replace(')', '').replace('\"', '').replace('\"', '').replace(',', '').replace('.', '')\\\n",
    "            .replace('?', '').replace('!', '').replace('\\ ', '').replace('/', '').replace('''''', '').replace('''' ''', '')\\\n",
    "            .replace(';', '').strip().lower() for x in jerry]\n",
    "    for x in beth:\n",
    "        try:\n",
    "            beth[beth.index(x)] = wuh[x]\n",
    "            next\n",
    "        except:\n",
    "            'KeyError'\n",
    "            next\n",
    "    return beth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whoitb(lst):\n",
    "    lmno = []\n",
    "    for x in lst:\n",
    "        n = leon.count_documents({'name':x})\n",
    "        lmno.append({x:n})\n",
    "    return lmno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nom = leon.distinct('name')\n",
    "wrdsmrty = smrt()\n",
    "famous = whoitb(nom)\n",
    "fms_ppl = {}\n",
    "for x in famous:\n",
    "    fms_ppl.update(x)\n",
    "type(fms_ppl), fms_ppl\n",
    "peps = pd.DataFrame(fms_ppl.values(), index=fms_ppl.keys()).sort_values(by=0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [go.Bar(y=peps[0], x=peps.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = go.Figure(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure({\n",
      "    'data': [{'type': 'bar',\n",
      "              'x': array(['Rick', 'Morty', 'Beth', 'Jerry', 'Summer', 'Pickle Rick', 'Supernova',\n",
      "                          'Cop Morty', 'All Ricks', 'Mr. Goldenfold', 'President', 'Cop Rick',\n",
      "                          'Testicle Monster A', 'Principal Vagina', 'Cornvelious Daniel',\n",
      "                          'Snuffles', 'Dr. Wong', 'Drunk Rick', 'Agency Director', 'Alan',\n",
      "                          'Candidate Morty', 'Vance', 'Scary Terry', 'Jessica', 'All Mortys',\n",
      "                          'Million Ants', 'Riq IV', 'Ice-T', 'Morty 2', 'All Summers',\n",
      "                          'Campaign Manager Morty', 'Alien Doctor', 'Lizard Morty', 'Cromulon',\n",
      "                          'Nathan', 'Slick', 'Brad', 'Young Rick', 'Glasses Morty', 'Rick J-22',\n",
      "                          'Vet', 'Birdperson', 'Mrs. Pancakes', 'Announcer', 'Teacher Rick',\n",
      "                          'Morty 1', 'Narrator', 'Summer 1'], dtype=object),\n",
      "              'y': array([420, 347, 148, 132,  97,  77,  44,  34,  32,  28,  27,  26,  26,  25,\n",
      "                           22,  22,  21,  21,  20,  19,  18,  17,  17,  16,  15,  15,  13,  13,\n",
      "                           13,  13,  12,  12,  11,  10,  10,  10,  10,   9,   9,   9,   9,   9,\n",
      "                            8,   8,   8,   8,   8,   7], dtype=int64)}],\n",
      "    'layout': {'template': '...'}\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whchone(brry):\n",
    "    ssns = brry.distinct('season')\n",
    "    eps = [brry.find({'season':x}, {'_id':0}) for x in ssns]\n",
    "    tre = []\n",
    "    for x in eps:\n",
    "        q = []\n",
    "        for t in x:\n",
    "            q.append(t)\n",
    "            next\n",
    "        tre.append(q)\n",
    "        next\n",
    "    return tre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[656, 'Summer 1', 'What do you mean?']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gathers all of the lines for the character and returns a list of the character's lines\n",
    "\n",
    "def saywht(da_pep, da_lst):\n",
    "    da_info = list(da_lst.find({'name':da_pep}))\n",
    "    guds = [[x['index'], x['name'], x['line']] for x in da_info]\n",
    "    return guds\n",
    "\n",
    "saywht('Summer 1', leon)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 'Rick', \"Morty! You gotta come on. Jus'... you gotta come with me.\"],\n",
       " [1, 'Morty', 'What, Rick? Whatâ€™s going on?']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# used to gather all of the lines preceeding all of the lines of the desired character\n",
    "# and returns paired off lines with index, name and line\n",
    "\n",
    "def scrpt(da_pep, da_lst):\n",
    "    gitgud = saywht(da_pep, da_lst)\n",
    "    dobttr = []\n",
    "    for x in gitgud:\n",
    "        q = leon.find_one({'index':(int(x[0])-1)})\n",
    "        try:\n",
    "            dobttr.append([[q['index'], q['name'], q['line']], x])\n",
    "        except:\n",
    "            'ValueError'\n",
    "            dobttr.append([['None', 'None', 'None'], x])\n",
    "    return dobttr\n",
    "\n",
    "mny = scrpt('Morty', leon)\n",
    "mny[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather all possible words (including names of characters) and put them into\n",
    "# a single list and create a tokenizer\n",
    "\n",
    "def roy(tknz):\n",
    "    splf = str('')\n",
    "    nchps = str('')\n",
    "    for x in tknz:\n",
    "        splf += x[0][1]+' '+x[0][2]\n",
    "        nchps += x[1][1]+' '+x[1][2]\n",
    "    splfnchps = splf+' '+nchps\n",
    "    ftbl = splfnchps.split(' ')\n",
    "    bcktocrpt = [x.replace('(', '').replace(')', '').replace('\"', '').replace('\"', '').replace(',', '').replace('.', '')\\\n",
    "        .replace('?', '').replace('!', '').replace('\\ ', '').replace('/', '').replace('''''', '').replace('''' ''', '')\\\n",
    "        .replace(';', '').strip().lower() for x in ftbl]\n",
    "    for z in bcktocrpt:\n",
    "        try:\n",
    "            bcktocrpt[bcktocrpt.index(z)] = wuh[z]\n",
    "            next\n",
    "        except:\n",
    "            'KeyError'\n",
    "            next\n",
    "    return bcktocrpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique words only \n",
    "\n",
    "def schmkl(twntyfv):\n",
    "    lmno = roy(twntyfv)\n",
    "    hijk = list(set(lmno))\n",
    "    return hijk[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = roy(mny)\n",
    "o = {x : p.count(x) for x in p}\n",
    "pqr = pd.DataFrame(o.values(), index=o.keys())\n",
    "stuv = pqr.sort_values(0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tokenzier\n",
    "\n",
    "def gimme(yowrds):\n",
    "    wrd = list(set(yowrds))\n",
    "    meaning = tf.keras.preprocessing.text.Tokenizer(num_words=len(wrd), filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\\\n",
    "                                             , lower=True, char_level=False\\\n",
    "                                             , oov_token='<<IDK>>')\n",
    "    meaning.fit_on_texts(yowrds)\n",
    "    return meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['morty', 'you', 'gotta']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gitgud = gimme(wrdsmrty)\n",
    "wrdsmrty[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrty = scrpt('Morty', leon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clnit(lines):\n",
    "    ftbl = splfnchps.split(' ')\n",
    "    bcktocrpt = [x.replace('(', '').replace(')', '').replace('\"', '').replace('\"', '').replace(',', '').replace('.', '')\\\n",
    "        .replace('?', '').replace('!', '').replace('\\ ', '').replace('/', '').replace('''''', '').replace('''' ''', '')\\\n",
    "        .replace(';', '').strip().lower() for x in ftbl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turns all of the lines and names into seqs\n",
    "# returns a list with target line first followed by\n",
    "# the character who spoke the preceeding line\n",
    "# as well as the preceeding line\n",
    "\n",
    "def matrx(red, blu):\n",
    "    lmno = []\n",
    "    for x in red:\n",
    "        ox = [y.replace('(', '').replace(')', '').replace('\"', '').replace('\"', '').replace(',', '').replace('.', '')\\\n",
    "              .replace('?', '').replace('!', '').replace('\\ ', '').replace('/', '').replace('''''', '').replace('''' ''', '')\\\n",
    "              .replace(';', '').strip().lower() for y in x[0][2].split(' ')]\n",
    "        q = blu.texts_to_sequences(ox)\n",
    "        xo = [w.replace('(', '').replace(')', '').replace('\"', '').replace('\"', '').replace(',', '').replace('.', '')\\\n",
    "              .replace('?', '').replace('!', '').replace('\\ ', '').replace('/', '').replace('''''', '').replace('''' ''', '')\\\n",
    "              .replace(';', '').strip().lower() for w in x[1][1].split(' ')]\n",
    "        qr = blu.texts_to_sequences(xo)\n",
    "        xox = [z.replace('(', '').replace(')', '').replace('\"', '').replace('\"', '').replace(',', '').replace('.', '')\\\n",
    "              .replace('?', '').replace('!', '').replace('\\ ', '').replace('/', '').replace('''''', '').replace('''' ''', '')\\\n",
    "              .replace(';', '').strip().lower() for z in x[1][2].split(' ')]\n",
    "        st = blu.texts_to_sequences(xox)\n",
    "        p = []\n",
    "        o = []\n",
    "        po = []\n",
    "        for y in q:\n",
    "            p += y\n",
    "            next\n",
    "        for z in qr:\n",
    "            o += z\n",
    "            next\n",
    "        for w in st:\n",
    "            po += w\n",
    "            next\n",
    "        uv = [po, p, o]\n",
    "        lmno.append(uv)\n",
    "    return lmno\n",
    "\n",
    "def bbb(ur_list):\n",
    "    datlyfe = [x[0] for x in ur_list]\n",
    "    thang = [w[1] for w in ur_list]\n",
    "    getswl = tf.keras.preprocessing.sequence.pad_sequences(datlyfe, maxlen=3695, dtype='int32', padding='post', value=(3695+53))\n",
    "    dyl = tf.keras.preprocessing.sequence.pad_sequences(thang, maxlen=3695, dtype='int32', padding='post', value=(3695+53))\n",
    "    legday = []\n",
    "    for y, x in enumerate(getswl):\n",
    "        legday.append([x, dyl[y]])\n",
    "        next\n",
    "    return legday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = matrx(mrty, gitgud)\n",
    "ba = bbb(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"morty you gotta come on jus' you gotta come with me                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \",\n",
       " array([  14,    2,  211, ..., 3748, 3748, 3748]))"
      ]
     },
     "execution_count": 760,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gitgud.sequences_to_texts(ba[0])[1].replace('<<IDK>>', ''), ba[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrds = tf.data.Dataset.from_tensor_slices(ba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thicc(twocs):\n",
    "    twocs = twocs[1]+twocs[0]\n",
    "    inpt = twocs[:3696]\n",
    "    target = twocs[3969::]\n",
    "    return inpt, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[  16   25  436 ... 3748 3748 3748]\n",
      " [  14    2  211 ... 3748 3748 3748]], shape=(2, 3695), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for x in wrds.take(1):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(3695,), dtype=int32, numpy=array([  30,   27,  647, ..., 7496, 7496, 7496])>, <tf.Tensor: shape=(0,), dtype=int32, numpy=array([], dtype=int32)>)\n"
     ]
    }
   ],
   "source": [
    "dataset = wrds.map(thicc)\n",
    "for cv in dataset.take(1):\n",
    "    print(cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {},
   "outputs": [],
   "source": [
    "buff_size = len(list(wrdsmrty))\n",
    "batch_size = 1\n",
    "\n",
    "dataset = wrds.shuffle(buff_size)\n",
    "# .batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 226    2   28 ... 3748 3748 3748]\n",
      " [1164  692 1903 ... 3748 3748 3748]], shape=(2, 3695), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for x in dataset.take(1):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(list(gitgud.word_index))\n",
    "embed_dim = 64\n",
    "rnn_units = 128\n",
    "\n",
    "def build_model(vocab_size, embed_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\\\n",
    "                                 tf.keras.layers.Embedding(vocab_size, embed_dim,\\\n",
    "                                                           batch_input_shape=[batch_size, None]),\\\n",
    "                                 tf.keras.layers.GRU(rnn_units,\\\n",
    "                                                      activation='tanh',\\\n",
    "                                                     return_sequences=True,\\\n",
    "                                                     stateful=True,\\\n",
    "                                                     recurrent_initializer='glorot_uniform'),\\\n",
    "                                tf.keras.layers.Dense(vocab_size, activation='softmax'),\\\n",
    "                                ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (1, None, 64)             232128    \n",
      "_________________________________________________________________\n",
      "gru_8 (GRU)                  (1, None, 128)            74496     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (1, None, 3627)           467883    \n",
      "=================================================================\n",
      "Total params: 774,507\n",
      "Trainable params: 774,507\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(vocab_size, embed_dim, rnn_units, batch_size)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(nochkn, nodinna):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(nochkn, nodinna, from_logits=False, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoints saved\n",
    "checkpoint_dir = 'LeonLikesTvShows'\n",
    "# files' names\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(dataset, epochs=5, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
